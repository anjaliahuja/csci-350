Title:  Writeup for Project 3, Fall 2015 CSCI 350
 Date:  11/08/2015
 Group:
 Anjali Ahuja 	ahujaa@usc.edu
 Anne Kao 		annekao@usc.edu
 Bernard Xie	bernardx@usc.edu


I. REQUIREMENTS:
	+ Part 1: 
	Implement system software-management of Translation Lookaside Buffer(TLB_.  It will be required to implement a TLB miss which will populate the TLB.  Will also have to make sure the TLB is set up properly on a context switch.
  Implement an inverted page table(IPT) which is supposed to keep track of what is in memory.  This will be updated when something is moved into/out of memory.

	+ Part 2:
	Take away preloading into memory.  This is implemented through not closing the executable and handling IPT misses.  On an IPT miss, the page will have to be loaded into memory.
  Implement Demand Paged Virtual Memory.  Memory has to be reduced thus memory will be filled up.  When memory is full, a page replacement policy must be implemented to remove pages.  In order to do this, the pages must be kept track of.
	
  + Part 3:
  Implement RPCs for all lock and condition variable systems
  Implement system for handling monitor variables
  System calls made by clients as user programs running on multiple instances of nachos, sent to one server who handles requests and sends back in form of message through RPCs 


II. ASSUMPTIONS:

	
III. DESIGN:
  i. Part 1
    Step 1: 
    Translation Lookaside Buffer(TLB): Cache of most recently used page entries.
    First we set up what happens on a PageFaultException.  When this happens we call populateTLB.  In this function we disable the interrupts (since we'll be dealing with the TLB) and calculate the relevant physical page number.  The data at that ppn gets stored into the TLB at the current index (global variable that cycles through 0, 1, 2, 3).  After this is done we disable the interrupts.  On a context switch (in restore state) we make sure to invalidate all valid tlb bits.  Also we make sure to remove all "machine->pageTable = pageTable".
    Step 2: 
    Inverted Page Table(IPT): Map for each page in physical memory to virtual memory.
    Setup IPT: We create a new struct that inherits from TranslationEntry.  We add an AddrSpace* addressSpace to it and then declare InvertedPageTable* ipt with a size of NumPhysPages.
    Everytime there is a "bitMap->Find()" we populate that same entry in the IPT.  This is done in both the AddrSpace constructor and in our AddrSpace::AllocateStack().  In exception.cc, we fill the TLB with the IPT data instead of the page table.
    We also update the invalidate the IPT valid bit on an exit_syscall.

  ii. Part 2
    Step 3: 
    Updated PageTable structure that inherits from TranslationEntry with an addition of a byteOffset (represent location of virtual page in executable/swap file) and disk location.  byteOffset is set as 40+i*pageSize and location is set as executable in the AddrSpace constructor.  On AllocateStack, the byteOffset is -1 and the location is NULL.
    Stop preloading into memory.  Everytime there is a "bitMap->Find()" we comment that section out and remove any code that uses the value (physical page number) from it.  This will be done in an IPT miss instead.
    If we search the IPT and did not find the physical page we needed to update the TLB, we've gotten an IPT miss.  On an IPT miss we allocate a new page of memory using "bitMap->Find()".  Then we go to the pageTable entry to find the needed virtual page in order to find out where the page is located on disk (is the byteOffset -1?).  If it's not there (is -1), then we must read a page from the executable.  When this is done, the PageTable's ppn is set and the valid bit is set to true.
    Step 4:
    We set NumPhysPages to 32 in machine.h.  Also the "Assert(numPages <= NumPhysPages)" is commented out.  We also added an List* iptQueue for FIFO page replacement policy.  Lock* iptLock for when we take from the iptQueue.  Openfile* swapfile which is the file that will be written to.  BitMap* swapMap which keeps track of where a particular page has been placed within the swapfile.  We added an enum for the PageReplacementPolicy that is set with -P to be RAND (random page selection) or FIFO (first in first out page selection).  Our PageTable struct created in Step 3 also now has a type that is either SWAP, EXECUTABLE, or NEITHER.  This will be used within handleIPTMiss to clear the bitmap when necessary.  Also, as discussed in class, on a context switch, we propogate the dirty bit.
    Within exit_syscall, we cycle through the virtual pages and clear the bitMap on each virtual page's corresponding physical page.  We also invalidate the IPT valid bit on that physical page and the page table on that virtual page.
    Because the NumPhysPages is decreased, this causes more times for the memory to be full, we add a handleMemoryFull function when a physical page cannot be found in the handleIPTmiss.  Inside this function we evict a page depending on whether the user passes in "-P RAND" or "-P FIFO".  We then check if the ppn is in the TLB.  If it is, we propogate the dirty bit and invalidate the TLB entry.  If the evicted page is dirty, it has to be copied into the swapfile.  We find a spot in the swapMap to write at.  When this is done, the page table must be updated to reflect the changes for the evicted page (byteOffset, type, and location if it was dirty, turn valid to false for all cases).
    The original handleIPTMiss was changed as well to incorporate the pageReplacementPolicy (add to iptQueue when necessary).  Now if we must read the page from the executable, we also check if the page is in the swapfile.  If it is, then we clear the swapMap of that page and set the current page's dirty bit to true.
    In populateTLB, the dirty bit must be propogated for the current tlb index.

  iii. Part 3
  New Remote procedure calls are added to the nettest.cc file. The server recieves a message from the client with which syscall it needs from the server, along with parameters needed for that syscall. The server then parses the message and enters into a switch statement to handle different requests.

  3 structs are added to the top of nettest, ServerLock, ServerCV and ServerMV. ServerLock stores the lock name, state, owner, wait queues for PacketHeader and MailHeader, whether the lock should be deleted, etc. The serverCV struct contains info about the lockIndex as well as retaining other information from project 2, and the serverMV struct holds the name, array of values, length of name and bool to be deleted.

  RPC Implementation:
  Create lock- takes in string stream of lock name, if the lock exists already, the index of the lock is sent back to client. If lock doesn't exist, a new lock is created and added to the vector of server locks, and the index of that lock is created. 
  Destroy Lock- destroy lock checks to see if the lock exists and that it isn't Null. If its available and the lock counter is at 0, the lock is destroyed and the lockID of the destroyed lock is sent back as confirmation.
  Acquire- Acquire checks if lock exists and that it isn't null, and that the lock owner is the one who is trying to acquire the lock, and that the lock state isn't busy. if lock state is busy, the owner is added to a wait queue of clients requesting to acquire the lock.
  Release- Release first does validation checks to make sure the lock is being released by the correct owner and the state is busy. It then checks if the wait queues of lock is empty, if it is it sets the lock to available. If it is not, it sends a message to the next client waiting for the lock that the lock is availble for them to use. 
  CreateCV- create CV will first check if the cv already exists, if it does it returns the index of the cv and increments the cv counter- the number of clients who create the cv. if its not, the cv is created, added to the server CV vector and the index is sent back to the client requesting it. 
  Destroy CV checks to see if the CV is not null and that it does exist, then checks to make sure that no client is using the CV still. If no client is using the CV, the CV is deleted and the client requesting the destroy is informed.
  Wait- RPC wait first checks that the lock waited on exists and is not null. Then it increments the use counter for the CVs and sets the lockIndex for the cv if needed. It adds the client mailbox and owner to the cvID packetwaiting and mailwaiting wait queues, then adds the lock associated with the cv to the packetWaiting and mailWaiting lock wait queues that are checked in release. Then, it checks to see whether the wait queues for the lockID are empty, if they aren't, it pops a client from the wait queues and sends the message to the receiving client. If they are empty, then the lock is set to available. 
  Signal- Signal first checks that the lock being signalled is a valid lock. Then it checks the state and that the owner is correct, then decrements the useCounter for the CV, and notifies the next waiting client in the SCVs mail and packet wait queues. 
  Broadcast- Broadcast does the same as signal, but signals each waiting client until the packet wait queue and mail wait queue for the SCVs vector is empty. 
  Create MV- create MV takes in the name and monitor variable size for the monitor varibale to be created. If it already exists, the name and index of the MV is returned. If it doesn't, the new mv is created with an array of values of size mvSize that is passed in by string stream. The MV is then added to the server MVs array and a message is sent with the mv index back to the requesting client.
  Destroy MV- destroy mvchecks to make sure the MV to be dsstroyed exists, if it does, it deletes the MV and replies with the ID of the deleted MV.
  Get MV- Get MV takes in the mvID and index and replies back the value of the MV at the requested index.
  Set MV- Set MV takes in the mvID, the mv index at that ID and the value to be set. It checks to make sure the MV exists, then sets the mv value at the index to the value sent in the string stream. It then replies with the value to  confirm the change. 
  Default- if the ss has an error, it will return the default "unKnown RPC" error message. 


 
IV. IMPLEMENTATION

  i. Files Modified
  WRITEUP
  ../code/machine/machine.h
  ../code/userprog/exception.cc
  ../code/threads/system.h
  ../code/threads/system.cc
  ../code/userprog/addrspace.h
  ../code/userprog/addrspace.cc
  ../code/userprog/progtest.cc
  ../code/test/Makefile
  ../code/network/nettest.cc


  ii. Files added
  ../code/test/matmult2exec.c
  ../code/test/matmult2fork.c
  ../code/test/sort2exec.c
  ../code/test/sort2fork.c
  ../code/test/MVtests.c
  ../code/test/MVtests1.c

  iii. Data Structures added, file they were added to
  In ../code/threads/system.h
    struct InvertedPageTable : public TranslationEntry {
     AddrSpace* addressSpace;
    };
    enum PageReplacementPolicy {
     RAND,
     FIFO
    };
    
    extern InvertedPageTable* ipt;
    extern PageReplacementPolicy pageReplacementPolicy;
    extern List* iptQueue;
    extern Lock* iptLock;

    #include "filesys.h"
    #define SwapSize 5000
    extern OpenFile* swapfile;
    extern BitMap* swapMap; 

  In ../code/threads/system.cc
    InvertedPageTable* ipt;
    PageReplacementPolicy pageReplacementPolicy;
    List* iptQueue;
    Lock* iptLock;
    OpenFile* swapfile;
    BitMap* swapMap;

  In ../code/userprog/addrspace.h
    enum FileType{
        EXECUTABLE,
        SWAP,
        NEITHER
    };
    struct PageTable : public TranslationEntry {
        int byteOffset;
        OpenFile* location;
        FileType type;
    };
    OpenFile* executable

  In ../code/network/nettest.cc
      enum ServerState{Busy, Available};

    Lock* lockLock = new Lock("Lock lock");
    Lock* CVLock = new Lock("CVLock");
    Lock* MVLock = new Lock("MVLock"); 

    struct ServerLock{
        string name;
        ServerState state;
        int owner;
        queue<PacketHeader*>* packetWaiting;
        queue<MailHeader*>* mailWaiting;
        bool toBeDeleted;
        int counter; 
    };

    struct ServerCV{
        string name;
        queue<PacketHeader*>* packetWaiting;
        queue<MailHeader*>* mailWaiting;
        int lockIndex;
        bool toBeDeleted;
        int counter;
        int useCounter;
    };

    struct ServerMV{
        string name;
        int* values;
        int len;
        bool toBeDeleted;
    };
     vector<ServerLock*>* SLocks = new vector<ServerLock*>;
    vector<ServerCV*>* SCVs = new vector<ServerCV*>;
    vector<ServerMV*>* SMVs = new vector<ServerMV*>; 



  iv. Data structures modified and file they were added to
  In ../code/userprog/addrspace.h
    TranslationEntry *pageTable --> PageTable *pageTable

  v. Functions added and in which file

  In ../code/nettest.cc
    void Server();
    void sendMessage(PacketHeader* outPktHdr, MailHeader* outMailheader, stringstream& msg);
  
  In ../code/userprog/exception.cc
    void populateTLB();
    int handleIPTMiss(int vpn);
    int handleMemoryFull();

  vi. Functions modified and in which file.
  In ../code/threads/system.cc
    Initialize(int argc, char **argv);

  In ../code/userprog/addrspace.cc
    AddrSpace::AddrSpace(OpenFile *executable);
    AddrSpace::SaveState();
    AddrSpace::RestoreState();
    AddrSpace::AllocateStack();

  In ../code/userprog/exception.cc
    Exec_Syscall(unsigned int vaddr, int len);
    Exit_Syscall(int status);
    ExceptionHandler(ExceptionType which);

  In ../code/userprog/progtest.cc
    StartProcess(char* filename);

V. TESTING
  Parts 1&2 tests must be run from /vm.  Part 3 tests must be run from /network.
  Part 1 & Part 2:
  In order to test parts 1 and 2 we created matmult2exec.c and sort2exec.c which exec 2 instances of matmult and sort respectively.  To run these tests with the random page replacement, type in "nachos -x ../test/matmult2exec -P RAND" and "nachos -x ../test/sort2exec -P RAND".  For FIFO, type in "nachos -x ../test/matmult2exec -P FIFO" and "nachos -x ../test/sort2exec -P FIFO".  
  We also created matmult2fork.c and sort2fork.c which fork 2 instances of matmult and sort respectively.  To run these tests with the random page replacement, type in "nachos -x ../test/matmult2fork -P RAND" and "nachos -x ../test/sort2fork -P RAND".  For FIFO, type in "nachos -x ../test/matmult2fork -P FIFO" and "nachos -x ../test/sort2fork -P FIFO".  
  matmult2 tests will print: 
    exit sys: 0
    exit sys: 7220
    exit sys: 7220
  The last two exit printouts come from the two instances of matmult.  The first printout comes from the main in the test.
  sort2 tests will print: 
    exit sys: 0
    exit sys: 1023
    exit sys: 1023
  The last two exit printouts come from the two instances of sort.  The first printout comes from the main in the test.
  Additional testing can occur by running a single instance of matmult and sort (in the instruction above, instead of matmult2 use matmult and instead of sort2 use sort).  This will print "exit sys: 7220" and "exit sys: 1023" respectively.

  Part 3:
  Tests for locks and cvs:

  have 6 terminal windows open
  server: run "nachos -m 0 -server"

  Test 1:
  This test shows that the first instance will acquire the lock.  
    see: Test 1 acquiring lock
         Test 1 acquires lock
  Each instance after will only say "Test 1 acquiring lock" because it is waiting on the lock to be released from the first instance.  When that happens, it will then say "Test 1 acquires lock".  
  terminal 1: run "nachos -m 1 -x ../test/lock_cv_test1"
  terminal 2: run "nachos -m 2 -x ../test/lock_cv_test1"
  terminal 3: run "nachos -m 3 -x ../test/lock_cv_test1"
  terminal 4: run "nachos -m 4 -x ../test/lock_cv_test1"
  terminal 5: run "nachos -m 5 -x ../test/lock_cv_test1"

  Test 2:
  This test demonstrates the wait, signal, and broadcast.  The first two instances will wait.  The third instance will signal, which will wake up the first waiting instance.  After that we have a fourth instance wait and the fifth instance will broadcast, waking up the second and fourth clients.
  terminal 1: run "nachos -m 1 -x ../test/lock_cv_test2"
  terminal 2: run "nachos -m 2 -x ../test/lock_cv_test2"
  terminal 3: run "nachos -m 3 -x ../test/lock_cv_test3"
  terminal 4: run "nachos -m 4 -x ../test/lock_cv_test2"
  terminal 5: run "nachos -m 5 -x ../test/lock_cv_test4"

  Test 3:
  This test demonstrates the destroying of a lock and cv.  The first instance will create a lock and cv and destroy it.  The second instance will also create the same lock and cv and then try to Acquire the lock and wait on the cv.  This will throw an error.
  See: 
    Test 6 tries to acquire lock and wait on cv (both destroyed)
    Error in acquiring lock
    Error in waiting cv
  terminal 1: run "nachos -m 1 -x ../test/lock_cv_test5"
  terminal 2: run "nachos -m 2 -x ../test/lock_cv_test6"


  Test 4: MV Tests
  This test demonstrates the RPC calls for monitor  variables that were created for this project (Create, get, set destroy). The first 4 instances of nachos will get an MV value, increment it and wait. The 5th instance will set a new value to the MV and broadcast the other 4 instances. After the instances are broadcasted, the new value of the monitor variable is printed and locks are released. 
  terminal1: run "nachos -m 0 -server"
  terminal2: run "nachos -m 1 -x ../test/MVtests"
  terminal3: run "nachos -m 2 -x ../test/MVtests"
  terminal4: run "nachos -m 3 -x ../test/MVtests"
  terminal5: run "nachos -m 4 -x ../test/MVtests"
  terminal6: run "nachos -m 5 -x ../test/MVtests1"


VI. DISCUSSION: 
	i. Experiment expectation 
  For parts 1 and 2 we expected matmult and sort to print out 7220 and 1023 respectively for the status in the exit syscall.

	ii. Experiment result
  For parts 1 and 2 the results meet the expectation.

	iii. Explanation (Explain how your project proves what its supposed to prove)
  For parts 1 and 2 we were able to use a TLB which is used to speed up address translation.  Through the implementation, we look at the TLB first which has a cache of pages.  Thus if we find what we are looking for a translation can be performed quickly.  We were also able to implement virtual memory using page replacement policies.

  For parts 3, we were successfully able to convert syscalls into RPCs so that the client and server can interact using messages, enabling multiple instances of nachos to be fun at the same time with shared variables. The tests prove that the syscalls that were working from project 2 correctly work as RPCs now in project 3.

VII. MISCELLANEOUS 

  
